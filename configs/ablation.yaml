# Ablation study: baseline without hierarchical pooling

# Data settings (same as default)
data:
  dataset_name: "BBBP"
  split_type: "scaffold"
  train_ratio: 0.8
  val_ratio: 0.1
  test_ratio: 0.1
  batch_size: 32
  num_workers: 4
  seed: 42

# Model settings - BASELINE VERSION
model:
  node_feat_dim: 9
  edge_feat_dim: 3
  hidden_dim: 128
  num_gnn_layers: 3
  gnn_type: "gcn"
  dropout: 0.2

  # Disable hierarchical attention pooling (key ablation)
  use_hierarchical_pooling: false
  num_attention_heads: 4
  functional_group_pooling: false  # Baseline uses global mean pooling
  scaffold_aware: false

  num_classes: 2
  task_type: "classification"

# Training settings (same as default)
training:
  num_epochs: 200
  learning_rate: 0.001
  weight_decay: 0.00001
  optimizer: "adam"
  scheduler: "cosine"
  scheduler_patience: 10
  scheduler_factor: 0.5

  # Disable curriculum learning for baseline
  use_curriculum: false
  curriculum_start_epoch: 0
  curriculum_end_epoch: 100

  gradient_clip: 1.0
  label_smoothing: 0.1
  mixup_alpha: 0.0

  early_stopping: true
  patience: 30
  min_delta: 0.0001

  use_amp: true

  save_dir: "checkpoints"
  save_best_only: true

  log_interval: 10
  use_mlflow: true

# Evaluation settings
evaluation:
  metrics:
    - "roc_auc"
    - "accuracy"
    - "f1"
    - "precision"
    - "recall"
  scaffold_split_eval: true
  save_predictions: true
  results_dir: "results"
